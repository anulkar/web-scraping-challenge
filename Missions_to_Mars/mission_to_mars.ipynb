{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitpythondatacondac30bd6fcf4694af7948ebff9c485f105",
   "display_name": "Python 3.7.5 64-bit ('PythonData': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use splinter create a new Chrome headless Browser instance\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser1 = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of Nasa Mars News Site that we want to scrape\n",
    "nasa_mars_news_url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "\n",
    "# Visit the Nasa Mars News Site\n",
    "browser1.visit(nasa_mars_news_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrape the [NASA Mars News](https://mars.nasa.gov/news/) Site and retrieve the latest News Title and Paragraph Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Here's the latest article from the Nasa Mars News site:\n--------------------------------------------------------\nNews Title: Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\nNews Teaser: NASA chose a seventh-grader from Virginia as winner of the agency's \"Name the Rover\" essay contest. Alexander Mather's entry for \"Perseverance\" was voted tops among 28,000 entries. \n--------------------------------------------------------\n"
    }
   ],
   "source": [
    "# Collect the latest News Title and Paragraph Text\n",
    "\n",
    "# Initialize lists to save titles and teasers (i.e. paragraph texts)\n",
    "# news_titles = []\n",
    "# news_teasers = []\n",
    "\n",
    "# Get the HTML content of the visited page\n",
    "nasa_mars_news_html = browser1.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with lxml parser\n",
    "nasa_news_soup = BeautifulSoup(nasa_mars_news_html, 'lxml')\n",
    "\n",
    "# print(f\"STARTING to Scrape the site: {nasa_mars_news_url}\")\n",
    "# print(\"---------------------------\")\n",
    "# # Loop through news listings\n",
    "# for news_listing in news_listings:\n",
    "try:\n",
    "#         # Retrieve the news title and save to a list\n",
    "    news_listing = nasa_news_soup.find('div', class_='list_text')\n",
    "    news_title = news_listing.find('div', class_='content_title').find('a').text\n",
    "    # news_titles.append(news_title)\n",
    "\n",
    "    # Retrieve the news paragraph text/teaser and save to a list\n",
    "    news_teaser = news_listing.find('div', class_='article_teaser_body').text\n",
    "    # news_teasers.append(news_teaser)\n",
    "    print(f\"Here's the latest article from the Nasa Mars News site:\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"News Title: {news_title}\")\n",
    "    print(f\"News Teaser: {news_teaser}\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "except AttributeError as e:\n",
    "    print(e)\n",
    "# print(f\"DONE with Scraping {len(news_titles)} News Titles and {len(news_teasers)} News Teasers\")\n",
    "# print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the image URL for the current Featured Mars Image on the [JPL Mars Space Images](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars) page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of JPL Mars Space Images Site that we want to scrape\n",
    "jpl_mars_images_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "# Visit the JPL Mars Space Images Site using splinter to create a new Chrome headless Browser instance\n",
    "browser2 = Browser('chrome', **executable_path, headless=False)\n",
    "browser2.visit(jpl_mars_images_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found the following Featured Image on the JPL Mars Space Images site: https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA16726_ip.jpg\n"
    }
   ],
   "source": [
    "# Get the HTML content of the visited page\n",
    "jpl_mars_images_html = browser2.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with lxml parser\n",
    "jpl_images_soup = BeautifulSoup(jpl_mars_images_html, 'lxml')\n",
    "\n",
    "# Find the image url for the current Featured Mars Image\n",
    "try:\n",
    "    featured_image_href = jpl_images_soup.find('div', class_='default floating_text_area ms-layer').find('footer').find('a')['data-fancybox-href']\n",
    "except AttributeError as e:\n",
    "    print(e)\n",
    "\n",
    "# Parse the JPL website URL and save a complete url string for the feature image\n",
    "parsed_uri = urlparse(jpl_mars_images_url)\n",
    "featured_image_url = f'{parsed_uri.scheme}://{parsed_uri.netloc}{featured_image_href}'\n",
    "print(f\"Found the following Featured Image on the JPL Mars Space Images site: {featured_image_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visit the [Mars Weather twitter account](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet containing the weather report tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of Twitter page to be scraped\n",
    "mars_twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "browser3 = Browser('chrome', **executable_path, headless=False)\n",
    "browser3.visit(mars_twitter_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Here is the latest weather report tweet from Mars Twitter\n----------------------------------------------------------\nsol 455 (2020-03-08) low -95.4ºC (-139.8ºF) high -13.0ºC (8.5ºF)\nwinds from the SSE at 6.0 m/s (13.5 mph) gusting to 20.7 m/s (46.2 mph)\npressure at 6.40 hPa\n"
    }
   ],
   "source": [
    "# Get the HTML content of the visited page\n",
    "mars_twitter_html = browser3.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with lxml parser\n",
    "mars_twitter_soup = BeautifulSoup(mars_twitter_html, 'lxml')\n",
    "\n",
    "# Retrieve the span tag for the specific class that holds all the tweet texts\n",
    "mars_weather_tweets = mars_twitter_soup.find_all('span', class_='css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0')\n",
    "\n",
    "# Loop through each tweet resultset\n",
    "for mars_weather_tweet in mars_weather_tweets: \n",
    "    try:\n",
    "        # Convert tweet text to lower case\n",
    "        mars_weather = mars_weather_tweet.text\n",
    "        lcase_mars_weather = mars_weather.lower()\n",
    "\n",
    "        # All Mars Weather tweets begin with the text \"insight sol\", so we only look for those tweets\n",
    "        if lcase_mars_weather.startswith('insight sol'):\n",
    "            # Remove the \"InSight\" word from the tweet text\n",
    "            mars_weather = mars_weather.replace(\"InSight \", \"\")\n",
    "            # Print the weather tweet and break from loop\n",
    "            print(\"Here is the latest weather report tweet from Mars Twitter\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(mars_weather)\n",
    "            break\n",
    "        else:\n",
    "            # Continue looping until we find the weather tweet\n",
    "            continue\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visit the [Mars Facts webpage](https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n    </tr>\n    <tr>\n      <th>Description</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Equatorial Diameter:</th>\n      <td>6,792 km</td>\n    </tr>\n    <tr>\n      <th>Polar Diameter:</th>\n      <td>6,752 km</td>\n    </tr>\n    <tr>\n      <th>Mass:</th>\n      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n    </tr>\n    <tr>\n      <th>Moons:</th>\n      <td>2 (Phobos &amp; Deimos)</td>\n    </tr>\n    <tr>\n      <th>Orbit Distance:</th>\n      <td>227,943,824 km (1.38 AU)</td>\n    </tr>\n    <tr>\n      <th>Orbit Period:</th>\n      <td>687 days (1.9 years)</td>\n    </tr>\n    <tr>\n      <th>Surface Temperature:</th>\n      <td>-87 to -5 °C</td>\n    </tr>\n    <tr>\n      <th>First Record:</th>\n      <td>2nd millennium BC</td>\n    </tr>\n    <tr>\n      <th>Recorded By:</th>\n      <td>Egyptian astronomers</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                              Value\nDescription                                        \nEquatorial Diameter:                       6,792 km\nPolar Diameter:                            6,752 km\nMass:                 6.39 × 10^23 kg (0.11 Earths)\nMoons:                          2 (Phobos & Deimos)\nOrbit Distance:            227,943,824 km (1.38 AU)\nOrbit Period:                  687 days (1.9 years)\nSurface Temperature:                   -87 to -5 °C\nFirst Record:                     2nd millennium BC\nRecorded By:                   Egyptian astronomers"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of Mars Facts webpage to be scraped\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "\n",
    "# Use the `read_html` function in Pandas to automatically scrape tabular data from the page\n",
    "# What we get in return is a list of DataFrames for any tabular data that Pandas found\n",
    "mars_facts_tables = pd.read_html(mars_facts_url)\n",
    "\n",
    "# Slice off the first Mars Facts DataFrame that we want using normal indexing\n",
    "mars_facts_df = mars_facts_tables[0]\n",
    "\n",
    "# Rename the DataFrame columns\n",
    "mars_facts_df.columns = ['Description', 'Value']\n",
    "\n",
    "# Set the index to the Description column and print to verify\n",
    "mars_facts_df.set_index('Description', inplace=True)\n",
    "mars_facts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visit the [USGS Astrogeology site](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mars' Hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of USGS Astrogeology site to be scraped\n",
    "usgs_astrogeo_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "browser4 = Browser('chrome', **executable_path, headless=False)\n",
    "browser4.visit(usgs_astrogeo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[{'title': 'Cerberus Hemisphere Enhanced', 'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]\n"
    }
   ],
   "source": [
    "# Dictionary and List to hold the image URLs and Titles of Mars' Hemispheres\n",
    "\n",
    "mars_hemisphere_image_dict = {}\n",
    "mars_hemisphere_image_urls = []\n",
    "\n",
    "# Iterate through each of the 4 Mars' Hemispheres pages\n",
    "for x in range(4):\n",
    "    try:\n",
    "        # Click the link to go to the appropriate Hemisphere's \"Enhanced\" page \n",
    "        if x == 0:    \n",
    "            browser4.click_link_by_partial_text('Cerberus')\n",
    "        elif x == 1:\n",
    "            browser4.click_link_by_partial_text('Schiaparelli')\n",
    "        elif x == 2:\n",
    "            browser4.click_link_by_partial_text('Syrtis')\n",
    "        else:\n",
    "            browser4.click_link_by_partial_text('Valles Marineris')\n",
    "\n",
    "        # Get the HTML content of the visited page\n",
    "        usgs_astrogeo_html = browser4.html\n",
    "\n",
    "        # Create BeautifulSoup object; parse with lxml parser\n",
    "        usgs_astrogeo_soup = BeautifulSoup(usgs_astrogeo_html, 'lxml')\n",
    "\n",
    "        # Retrieve the full image URL and title of the hemisphere\n",
    "        full_image_url = usgs_astrogeo_soup.find('div', class_='downloads').find('li').find('a')['href']\n",
    "        mars_hemispheres_title = usgs_astrogeo_soup.find('h2', class_='title').text\n",
    "\n",
    "        # Store the image URL and title into a dictionary and then append it to a list\n",
    "        mars_hemisphere_image_dict = {\"title\": mars_hemispheres_title, \"img_url\": full_image_url}\n",
    "        mars_hemisphere_image_urls.append(dict(mars_hemisphere_image_dict))\n",
    "\n",
    "        # Go back to the main page so we can click on the next Hemisphere page link\n",
    "        browser4.back()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(mars_hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}